{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PRU import PRU_trace, TamakiOptimizer\n",
    "from eunn import Unitary\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_PRU():\n",
    "    \n",
    "    n_batch = 1\n",
    "    n_qubits = 10\n",
    "    n_layers = 6\n",
    "    epochs = 2\n",
    "\n",
    "    pru = PRU_trace(n_qubits, n_layers, optimizer=TamakiOptimizer(wait_time=min(2**min(n_qubits,n_layers), 1500)))\n",
    "    even_layers = (n_layers+1)//2\n",
    "    odd_layers = n_layers//2\n",
    "    even_counts = n_qubits//2\n",
    "    odd_counts = (n_qubits-1)//2\n",
    "    gates = ( even_layers * even_counts + odd_layers * odd_counts ) * 2\n",
    "    unitary_gen = Unitary(n_batch * gates)\n",
    "    torch_optim = torch.optim.Adam(unitary_gen.parameters(), lr=0.1)\n",
    "    index1 = even_layers*even_counts\n",
    "    index2 = index1 + odd_layers*odd_counts\n",
    "    index3 = index2 + index1\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        unitaries = unitary_gen()\n",
    "        unitaries = unitaries.reshape(n_batch, gates, 2, 2, 2, 2)\n",
    "        unitary_dict = {}\n",
    "        unitary_dict['even_layer_unitaries1'] = unitaries[:, :index1].reshape(n_batch, even_layers, even_counts, 2, 2, 2, 2)\n",
    "        unitary_dict['odd_layer_unitaries1'] = unitaries[:, index1:index2].reshape(n_batch, odd_layers, odd_counts, 2, 2, 2, 2)\n",
    "        unitary_dict['even_layer_unitaries2'] = unitaries[:, index2:index3].reshape(n_batch, even_layers, even_counts, 2, 2, 2, 2)\n",
    "        unitary_dict['odd_layer_unitaries2'] = unitaries[:, index3:].reshape(n_batch, odd_layers, odd_counts, 2, 2, 2, 2)\n",
    "        #unitary_dict = pru.layer_random_unitaries('cpu', n_batch)\n",
    "        results = pru(unitary_dict)\n",
    "        loss = - (results.abs()**6).mean()\n",
    "        torch_optim.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        print(unitary_gen.angles[0])\n",
    "        #print(loss)\n",
    "        torch_optim.step()\n",
    "        value = unitaries[0,0].reshape(4,4)\n",
    "        #print(value)\n",
    "        unitary1 = unitary_dict['even_layer_unitaries1'][0,-1,-1].reshape(4,4)\n",
    "        unitary2 = unitary_dict['even_layer_unitaries2'][0,-1,-1].reshape(4,4)\n",
    "        #print(abs(unitary1)-abs(unitary2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using previously saved contraction order at  <_io.BufferedReader name='/home/minzhao.liu/Saved_Contraction_Orders/TamakiOptimizer/TraceEvaluation/wait_time_64/PRU_trace_n_10_l_6.pickle'>\n",
      "tensor([[3.9192, 4.6635, 5.3250, 2.6761],\n",
      "        [4.8792, 3.1118, 1.4148, 3.7182],\n",
      "        [6.1518, 3.1189, 6.1233, 2.0921],\n",
      "        [2.9336, 6.0892, 3.3753, 2.4000]], grad_fn=<SelectBackward0>)\n",
      "tensor([[4.0192, 4.7635, 5.4250, 2.7761],\n",
      "        [4.9792, 3.2118, 1.3148, 3.6182],\n",
      "        [6.2518, 3.0189, 6.2233, 1.9921],\n",
      "        [2.8336, 5.9892, 3.2753, 2.5000]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "train_PRU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4755, grad_fn=<MeanBackward0>)\n",
      "tensor(0.4073, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3484, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3276, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3186, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2907, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3007, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2962, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2678, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2899, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 1.3670e-08, 8.3071e-08, 4.1650e-09],\n",
       "        [1.3670e-08, 1.0000e+00, 5.6650e-09, 7.6799e-09],\n",
       "        [8.3071e-08, 5.6650e-09, 1.0000e+00, 4.2742e-09],\n",
       "        [4.1650e-09, 7.6799e-09, 4.2742e-09, 1.0000e+00]],\n",
       "       grad_fn=<AbsBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "unitary_gen = Unitary(1)\n",
    "torch_optim = torch.optim.Adam(unitary_gen.parameters(), lr=0.2)\n",
    "\n",
    "for _ in range(epochs):\n",
    "    a = unitary_gen()\n",
    "    loss = a.abs().mean()\n",
    "    torch_optim.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    print(loss)\n",
    "    torch_optim.step()\n",
    "\n",
    "a = a.squeeze(0)\n",
    "(a@a.T.conj()).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e37f52b1ac3a33a4d187d5ade9b87bf0015f2320ccb2f09d6ad5cc3e492454b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('qtensor-torch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
